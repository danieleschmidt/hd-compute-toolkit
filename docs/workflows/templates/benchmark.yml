# Performance Benchmarking Workflow Template
# Copy this to .github/workflows/benchmark.yml

name: Performance Benchmarks

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  schedule:
    - cron: '0 2 * * 1'  # Weekly benchmarks on Monday

env:
  PYTHON_VERSION: '3.10'

jobs:
  cpu-benchmarks:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark psutil
    
    - name: Run CPU benchmarks
      run: |
        pytest tests/performance/ -m benchmark --benchmark-json=cpu-benchmark.json
    
    - name: Store benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: cpu-benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: true

  gpu-benchmarks:
    runs-on: [self-hosted, gpu]
    if: github.repository == 'yourusername/hd-compute-toolkit'  # Only run on main repo
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Check CUDA availability
      run: |
        nvidia-smi
        python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark psutil
        pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
    
    - name: Run GPU benchmarks
      run: |
        pytest tests/performance/ -m "benchmark and gpu" --benchmark-json=gpu-benchmark.json
    
    - name: Store GPU benchmark results
      uses: benchmark-action/github-action-benchmark@v1
      with:
        tool: 'pytest'
        output-file-path: gpu-benchmark.json
        github-token: ${{ secrets.GITHUB_TOKEN }}
        auto-push: true
        comment-on-alert: true
        alert-threshold: '150%'
        fail-on-alert: true

  memory-benchmarks:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark psutil memory-profiler
    
    - name: Run memory benchmarks
      run: |
        pytest tests/performance/ -m memory --benchmark-json=memory-benchmark.json
    
    - name: Generate memory report
      run: |
        echo "## Memory Benchmark Results" >> $GITHUB_STEP_SUMMARY
        echo "Memory usage and efficiency tests completed" >> $GITHUB_STEP_SUMMARY

  regression-detection:
    runs-on: ubuntu-latest
    if: github.event_name == 'pull_request'
    steps:
    - uses: actions/checkout@v4
      with:
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -e ".[dev]"
        pip install pytest-benchmark
    
    - name: Run benchmarks on current branch
      run: |
        pytest tests/performance/ -m benchmark --benchmark-json=current-benchmark.json
    
    - name: Checkout main branch
      run: |
        git checkout main
        pip install -e ".[dev]"
    
    - name: Run benchmarks on main branch
      run: |
        pytest tests/performance/ -m benchmark --benchmark-json=main-benchmark.json
    
    - name: Compare performance
      run: |
        python -c "
        import json
        
        with open('current-benchmark.json') as f:
            current = json.load(f)
        with open('main-benchmark.json') as f:
            main = json.load(f)
        
        print('Performance comparison results would be analyzed here')
        # Implement performance regression detection logic
        "

  benchmark-report:
    runs-on: ubuntu-latest
    needs: [cpu-benchmarks, memory-benchmarks]
    if: always()
    steps:
    - name: Generate benchmark report
      run: |
        echo "## ðŸ“Š Performance Benchmark Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "- CPU benchmarks: ${{ needs.cpu-benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Memory benchmarks: ${{ needs.memory-benchmarks.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "Full benchmark results are available in the workflow artifacts." >> $GITHUB_STEP_SUMMARY