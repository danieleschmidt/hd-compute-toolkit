# GPU Testing Workflow
# Runs comprehensive GPU tests on self-hosted runners with NVIDIA GPUs

name: GPU Tests

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'hd_compute/**'
      - 'tests/**'
      - 'pyproject.toml'
  pull_request:
    branches: [ main ]
    paths:
      - 'hd_compute/**'
      - 'tests/**'
  schedule:
    # Run nightly GPU tests
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      test_suite:
        description: 'GPU test suite to run'
        required: false
        default: 'all'
        type: choice
        options:
          - all
          - unit
          - integration
          - performance

env:
  PYTHONUNBUFFERED: 1
  CUDA_VISIBLE_DEVICES: 0

jobs:
  gpu-compatibility-check:
    runs-on: [self-hosted, gpu, nvidia]
    
    strategy:
      matrix:
        cuda-version: ['11.8', '12.1']
        python-version: ['3.9', '3.10', '3.11']
        pytorch-version: ['2.0', '2.1']
      fail-fast: false

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check GPU availability
        run: |
          nvidia-smi
          echo "CUDA Version: $(nvcc --version)"
          echo "Available GPUs: $(nvidia-smi --list-gpus)"

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install CUDA toolkit
        run: |
          wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb
          sudo dpkg -i cuda-keyring_1.0-1_all.deb
          sudo apt-get update
          sudo apt-get -y install cuda-toolkit-${{ matrix.cuda-version }}

      - name: Install PyTorch with CUDA
        run: |
          pip install torch==${{ matrix.pytorch-version }}.* torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,vulkan]"

      - name: Verify CUDA setup
        run: |
          python -c "
          import torch
          print(f'PyTorch version: {torch.__version__}')
          print(f'CUDA available: {torch.cuda.is_available()}')
          print(f'CUDA version: {torch.version.cuda}')
          print(f'GPU count: {torch.cuda.device_count()}')
          if torch.cuda.is_available():
              for i in range(torch.cuda.device_count()):
                  print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
                  print(f'Memory: {torch.cuda.get_device_properties(i).total_memory / 1024**3:.1f} GB')
          "

      - name: Run GPU unit tests
        if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'unit' }}
        run: |
          python -m pytest tests/unit/ -m gpu -v \
            --cov=hd_compute --cov-report=xml:coverage-gpu-unit.xml \
            --junitxml=test-results-gpu-unit.xml

      - name: Run GPU integration tests
        if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'integration' }}
        run: |
          python -m pytest tests/integration/ -m gpu -v \
            --cov=hd_compute --cov-append --cov-report=xml:coverage-gpu-integration.xml \
            --junitxml=test-results-gpu-integration.xml

      - name: Run GPU performance benchmarks
        if: ${{ github.event.inputs.test_suite == 'all' || github.event.inputs.test_suite == 'performance' }}
        run: |
          python -m pytest tests/performance/ -m gpu --benchmark-only \
            --benchmark-json=benchmark-results-gpu.json

      - name: Upload test results
        uses: actions/upload-artifact@v3
        if: always()
        with:
          name: gpu-test-results-${{ matrix.cuda-version }}-py${{ matrix.python-version }}
          path: |
            test-results-*.xml
            coverage-*.xml
            benchmark-results-*.json

      - name: Memory stress test
        run: |
          python -c "
          import torch
          import hd_compute
          
          # Test memory scaling
          dimensions = [1000, 5000, 10000, 20000]
          for dim in dimensions:
              try:
                  hdc = hd_compute.HDCompute(dim=dim, device='cuda')
                  vectors = [hdc.random_hv() for _ in range(100)]
                  result = hdc.bundle(vectors)
                  print(f'‚úÖ Dimension {dim}: Success')
                  
                  # Check memory usage
                  allocated = torch.cuda.memory_allocated() / 1024**2
                  reserved = torch.cuda.memory_reserved() / 1024**2
                  print(f'   Memory: {allocated:.1f}MB allocated, {reserved:.1f}MB reserved')
                  
                  torch.cuda.empty_cache()
              except Exception as e:
                  print(f'‚ùå Dimension {dim}: {e}')
          "

  multi-gpu-tests:
    runs-on: [self-hosted, gpu, multi-gpu]
    if: contains(github.event.head_commit.message, '[multi-gpu]') || github.event_name == 'schedule'
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check multi-GPU setup
        run: |
          nvidia-smi
          python -c "
          import torch
          print(f'Available GPUs: {torch.cuda.device_count()}')
          for i in range(torch.cuda.device_count()):
              print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
          "

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Test distributed operations
        run: |
          python -c "
          import torch
          import torch.multiprocessing as mp
          from hd_compute.distributed import DistributedHDC
          
          def test_distributed():
              if torch.cuda.device_count() < 2:
                  print('Skipping multi-GPU test: Need at least 2 GPUs')
                  return
                  
              dhdc = DistributedHDC(
                  dim=10000,
                  num_gpus=min(2, torch.cuda.device_count()),
                  backend='nccl'
              )
              
              # Test parallel encoding
              test_data = [torch.randn(1000) for _ in range(100)]
              encoded = dhdc.parallel_encode(test_data)
              print(f'‚úÖ Distributed encoding successful: {encoded.shape}')
          
          test_distributed()
          "

  fpga-acceleration-tests:
    runs-on: [self-hosted, fpga]
    if: contains(github.event.head_commit.message, '[fpga]')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check FPGA availability
        run: |
          # Check for Xilinx/Intel FPGA tools
          which vivado || echo "Vivado not found"
          which quartus || echo "Quartus not found"
          lspci | grep -i fpga || echo "No FPGA devices found"

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,fpga]"

      - name: Test FPGA acceleration
        run: |
          python -c "
          try:
              from hd_compute.kernels import FPGAAccelerator
              
              accelerator = FPGAAccelerator(
                  bitstream='test_kernels/hdc_bundle.bit',
                  clock_freq=200e6
              )
              
              # Test accelerated operations
              import torch
              vectors = [torch.randint(0, 2, (10000,), dtype=torch.bool) for _ in range(100)]
              result = accelerator.bundle_batch(vectors, batch_size=100)
              print(f'‚úÖ FPGA acceleration test passed: {result.shape}')
              
          except ImportError:
              print('‚ö†Ô∏è FPGA acceleration not available')
          except Exception as e:
              print(f'‚ùå FPGA test failed: {e}')
          "

  vulkan-compute-tests:
    runs-on: [self-hosted, vulkan]
    if: contains(github.event.head_commit.message, '[vulkan]')
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check Vulkan support
        run: |
          vulkaninfo --summary || echo "Vulkan not available"
          python -c "
          try:
              import vulkan
              print('‚úÖ Python Vulkan bindings available')
          except ImportError:
              print('‚ùå Python Vulkan bindings not found')
          "

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev,vulkan]"

      - name: Test Vulkan compute shaders
        run: |
          python -c "
          try:
              from hd_compute.kernels.vulkan import VulkanCompute
              
              compute = VulkanCompute()
              
              # Test basic compute operations
              import numpy as np
              a = np.random.randint(0, 2, (10000,), dtype=np.uint32)
              b = np.random.randint(0, 2, (10000,), dtype=np.uint32)
              
              result = compute.bitwise_xor(a, b)
              print(f'‚úÖ Vulkan compute test passed: {result.shape}')
              
          except ImportError:
              print('‚ö†Ô∏è Vulkan compute not available')
          except Exception as e:
              print(f'‚ùå Vulkan test failed: {e}')
          "

  performance-regression-check:
    runs-on: [self-hosted, gpu, benchmark]
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[dev]"

      - name: Download baseline benchmarks
        run: |
          # Download previous benchmark results
          curl -H "Authorization: token ${{ secrets.GITHUB_TOKEN }}" \
               -H "Accept: application/vnd.github.v3.raw" \
               -o baseline-benchmarks.json \
               "https://api.github.com/repos/${{ github.repository }}/contents/benchmarks/baseline.json"

      - name: Run performance benchmarks
        run: |
          python -m pytest tests/performance/ -m "gpu and benchmark" \
            --benchmark-json=current-benchmarks.json \
            --benchmark-compare=baseline-benchmarks.json \
            --benchmark-compare-fail=min:5%

      - name: Generate performance report
        run: |
          python scripts/generate_perf_report.py \
            --baseline baseline-benchmarks.json \
            --current current-benchmarks.json \
            --output performance-report.md

      - name: Upload benchmark results
        uses: actions/upload-artifact@v3
        with:
          name: performance-benchmarks
          path: |
            current-benchmarks.json
            performance-report.md

      - name: Comment performance results
        uses: actions/github-script@v6
        if: github.event_name == 'pull_request'
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('performance-report.md', 'utf8');
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: '## üöÄ GPU Performance Report\n\n' + report
            });

  cleanup:
    runs-on: [self-hosted, gpu]
    needs: [gpu-compatibility-check, multi-gpu-tests, performance-regression-check]
    if: always()
    
    steps:
      - name: Cleanup GPU memory
        run: |
          python -c "
          import torch
          if torch.cuda.is_available():
              torch.cuda.empty_cache()
              print('GPU memory cache cleared')
          "

      - name: Cleanup Docker containers
        run: |
          docker system prune -f
          docker volume prune -f

      - name: Reset GPU state
        run: |
          sudo nvidia-smi --gpu-reset || echo "GPU reset not supported"