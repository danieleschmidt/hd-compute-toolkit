#!/usr/bin/env python3
"""
Comprehensive Monitoring and Health Check System for HD-Compute-Toolkit.

This module implements production-grade monitoring, health checks, 
metrics collection, and alerting for HDC operations.
"""

import time
import json
import os
from typing import Dict, List, Any, Optional, Callable
from dataclasses import dataclass, asdict
from datetime import datetime, timedelta
import logging

# Configure monitoring logger
monitor_logger = logging.getLogger('hdc_monitor')
monitor_logger.setLevel(logging.INFO)

@dataclass
class HealthMetrics:
    """Health metrics for HDC system."""
    timestamp: float
    cpu_usage_percent: float
    memory_usage_mb: float
    operations_per_second: float
    error_rate: float
    average_latency_ms: float
    active_connections: int
    status: str = "healthy"

@dataclass
class PerformanceAlert:
    """Performance alert definition."""
    timestamp: float
    alert_type: str
    severity: str
    message: str
    metric_value: float
    threshold: float

class HealthMonitor:
    """Comprehensive health monitoring for HDC operations."""
    
    def __init__(self, alert_thresholds: Optional[Dict[str, float]] = None):
        self.alert_thresholds = alert_thresholds or {
            'max_latency_ms': 1000.0,
            'max_error_rate': 0.05,
            'max_memory_mb': 1000.0,
            'min_ops_per_second': 10.0
        }
        
        self.metrics_history: List[HealthMetrics] = []
        self.alerts: List[PerformanceAlert] = []
        self.operation_times: List[float] = []
        self.error_count = 0
        self.operation_count = 0
        self.start_time = time.time()
        
        monitor_logger.info("Health monitor initialized with thresholds: %s", self.alert_thresholds)
    
    def record_operation(self, operation_time: float, success: bool = True):
        """Record an operation's performance."""
        self.operation_times.append(operation_time)
        self.operation_count += 1
        
        if not success:
            self.error_count += 1
        
        # Keep only recent operation times (last 1000)
        if len(self.operation_times) > 1000:
            self.operation_times = self.operation_times[-1000:]
    
    def get_current_metrics(self) -> HealthMetrics:
        """Get current system health metrics."""
        now = time.time()
        
        # Calculate performance metrics
        if self.operation_times:
            avg_latency_ms = (sum(self.operation_times) / len(self.operation_times)) * 1000
        else:
            avg_latency_ms = 0.0
        
        elapsed_time = now - self.start_time
        ops_per_second = self.operation_count / max(elapsed_time, 0.001)
        error_rate = self.error_count / max(self.operation_count, 1)
        
        # Simulate system metrics (in production, use psutil or similar)
        cpu_usage = min(ops_per_second * 2, 100.0)  # Simulated based on ops
        memory_usage = len(self.operation_times) * 0.1  # Simulated
        
        # Determine overall status
        status = self._determine_health_status(avg_latency_ms, error_rate, memory_usage, ops_per_second)
        
        metrics = HealthMetrics(
            timestamp=now,
            cpu_usage_percent=cpu_usage,
            memory_usage_mb=memory_usage,
            operations_per_second=ops_per_second,
            error_rate=error_rate,
            average_latency_ms=avg_latency_ms,
            active_connections=1,  # Simulated
            status=status
        )
        
        self.metrics_history.append(metrics)
        
        # Keep only recent metrics (last 100)
        if len(self.metrics_history) > 100:
            self.metrics_history = self.metrics_history[-100:]
        
        # Check for alerts
        self._check_alerts(metrics)
        
        return metrics
    
    def _determine_health_status(self, latency: float, error_rate: float, 
                                memory: float, ops_per_sec: float) -> str:
        """Determine overall health status."""
        if (latency > self.alert_thresholds['max_latency_ms'] or
            error_rate > self.alert_thresholds['max_error_rate'] or
            memory > self.alert_thresholds['max_memory_mb']):
            return "critical"
        elif (latency > self.alert_thresholds['max_latency_ms'] * 0.8 or
              error_rate > self.alert_thresholds['max_error_rate'] * 0.8 or
              ops_per_sec < self.alert_thresholds['min_ops_per_second']):
            return "warning"
        else:
            return "healthy"
    
    def _check_alerts(self, metrics: HealthMetrics):
        """Check for performance alerts."""
        alerts_triggered = []
        
        if metrics.average_latency_ms > self.alert_thresholds['max_latency_ms']:
            alerts_triggered.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type="high_latency",
                severity="critical" if metrics.average_latency_ms > self.alert_thresholds['max_latency_ms'] * 2 else "warning",
                message=f"High latency detected: {metrics.average_latency_ms:.1f}ms",
                metric_value=metrics.average_latency_ms,
                threshold=self.alert_thresholds['max_latency_ms']
            ))
        
        if metrics.error_rate > self.alert_thresholds['max_error_rate']:
            alerts_triggered.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type="high_error_rate",
                severity="critical",
                message=f"High error rate detected: {metrics.error_rate:.3f}",
                metric_value=metrics.error_rate,
                threshold=self.alert_thresholds['max_error_rate']
            ))
        
        if metrics.memory_usage_mb > self.alert_thresholds['max_memory_mb']:
            alerts_triggered.append(PerformanceAlert(
                timestamp=metrics.timestamp,
                alert_type="high_memory",
                severity="warning",
                message=f"High memory usage: {metrics.memory_usage_mb:.1f}MB",
                metric_value=metrics.memory_usage_mb,
                threshold=self.alert_thresholds['max_memory_mb']
            ))
        
        for alert in alerts_triggered:
            self.alerts.append(alert)
            monitor_logger.warning("ALERT [%s]: %s", alert.severity.upper(), alert.message)
    
    def get_health_summary(self) -> Dict[str, Any]:
        """Get comprehensive health summary."""
        current_metrics = self.get_current_metrics()
        recent_alerts = [alert for alert in self.alerts 
                        if alert.timestamp > time.time() - 300]  # Last 5 minutes
        
        return {
            'current_status': current_metrics.status,
            'current_metrics': asdict(current_metrics),
            'recent_alerts_count': len(recent_alerts),
            'recent_alerts': [asdict(alert) for alert in recent_alerts[-5:]],  # Last 5 alerts
            'uptime_seconds': time.time() - self.start_time,
            'total_operations': self.operation_count,
            'total_errors': self.error_count,
            'monitoring_active': True
        }
    
    def export_metrics(self, filepath: str):
        """Export metrics to file for external monitoring."""
        summary = self.get_health_summary()
        summary['export_timestamp'] = datetime.now().isoformat()
        summary['metrics_history'] = [asdict(m) for m in self.metrics_history[-50:]]
        
        try:
            with open(filepath, 'w') as f:
                json.dump(summary, f, indent=2)
            monitor_logger.info("Metrics exported to %s", filepath)
        except Exception as e:
            monitor_logger.error("Failed to export metrics: %s", e)

class MonitoredHDC:
    """HDC wrapper with comprehensive monitoring."""
    
    def __init__(self, backend_class, dim: int, device: Optional[str] = None, 
                 enable_monitoring: bool = True, **kwargs):
        self.backend = backend_class(dim=dim, device=device, **kwargs)
        self.enable_monitoring = enable_monitoring
        
        if self.enable_monitoring:
            self.monitor = HealthMonitor()
            monitor_logger.info("Monitoring enabled for HDC operations")
        else:
            self.monitor = None
    
    def _record_operation(self, operation_name: str, func: Callable, *args, **kwargs):
        """Execute operation with monitoring."""
        start_time = time.time()
        success = True
        
        try:
            result = func(*args, **kwargs)
            return result
        except Exception as e:
            success = False
            monitor_logger.error("Operation %s failed: %s", operation_name, e)
            raise
        finally:
            if self.monitor:
                operation_time = time.time() - start_time
                self.monitor.record_operation(operation_time, success)
    
    def random_hv(self, sparsity: float = 0.5) -> Any:
        """Generate random hypervector with monitoring."""
        if self.monitor:
            return self._record_operation("random_hv", self.backend.random_hv, sparsity=sparsity)
        else:
            return self.backend.random_hv(sparsity=sparsity)
    
    def bundle(self, hvs: List[Any]) -> Any:
        """Bundle hypervectors with monitoring."""
        if self.monitor:
            return self._record_operation("bundle", self.backend.bundle, hvs)
        else:
            return self.backend.bundle(hvs)
    
    def bind(self, hv1: Any, hv2: Any) -> Any:
        """Bind hypervectors with monitoring."""
        if self.monitor:
            return self._record_operation("bind", self.backend.bind, hv1, hv2)
        else:
            return self.backend.bind(hv1, hv2)
    
    def cosine_similarity(self, hv1: Any, hv2: Any) -> float:
        """Compute cosine similarity with monitoring."""
        if self.monitor:
            return self._record_operation("cosine_similarity", self.backend.cosine_similarity, hv1, hv2)
        else:
            return self.backend.cosine_similarity(hv1, hv2)
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get current health status."""
        if self.monitor:
            return self.monitor.get_health_summary()
        else:
            return {'monitoring_active': False, 'current_status': 'unknown'}
    
    def export_monitoring_data(self, filepath: str = "/tmp/hdc_metrics.json"):
        """Export monitoring data."""
        if self.monitor:
            self.monitor.export_metrics(filepath)
        else:
            monitor_logger.warning("Monitoring not enabled, cannot export data")

def run_monitoring_demo():
    """Demonstrate comprehensive monitoring capabilities."""
    print("ğŸ“Š Comprehensive Monitoring Demo")
    print("=" * 40)
    
    from hd_compute import HDComputePython
    
    # Initialize monitored HDC
    monitored_hdc = MonitoredHDC(HDComputePython, dim=1000, enable_monitoring=True)
    
    # Simulate various operations
    print("Running monitored operations...")
    
    # Normal operations
    for i in range(20):
        hv1 = monitored_hdc.random_hv()
        hv2 = monitored_hdc.random_hv()
        bundled = monitored_hdc.bundle([hv1, hv2])
        similarity = monitored_hdc.cosine_similarity(hv1, hv2)
        
        # Add some artificial delay to simulate processing
        time.sleep(0.01)
    
    # Simulate some errors
    print("Simulating error conditions...")
    try:
        # This will fail and be recorded
        monitored_hdc.bundle([])
    except Exception:
        pass
    
    try:
        # This will also fail
        monitored_hdc.cosine_similarity(None, None)
    except Exception:
        pass
    
    # Get health status
    health_status = monitored_hdc.get_health_status()
    print(f"âœ“ Health Status: {health_status['current_status']}")
    print(f"âœ“ Total Operations: {health_status['total_operations']}")
    print(f"âœ“ Error Count: {health_status['total_errors']}")
    print(f"âœ“ Error Rate: {health_status['current_metrics']['error_rate']:.3f}")
    print(f"âœ“ Average Latency: {health_status['current_metrics']['average_latency_ms']:.1f}ms")
    print(f"âœ“ Operations/sec: {health_status['current_metrics']['operations_per_second']:.1f}")
    
    # Export monitoring data
    export_path = "/tmp/hdc_monitoring_demo.json"
    monitored_hdc.export_monitoring_data(export_path)
    
    if os.path.exists(export_path):
        print(f"âœ“ Monitoring data exported to {export_path}")
        with open(export_path, 'r') as f:
            data = json.load(f)
        print(f"âœ“ Export contains {len(data.get('metrics_history', []))} historical metrics")
    
    # Show recent alerts
    if health_status['recent_alerts_count'] > 0:
        print(f"âš  Recent alerts: {health_status['recent_alerts_count']}")
        for alert in health_status['recent_alerts']:
            print(f"  - {alert['alert_type']}: {alert['message']}")
    else:
        print("âœ“ No recent alerts")
    
    return True

def run_stress_test():
    """Run stress test to trigger monitoring alerts."""
    print("\nğŸ”¥ Stress Test for Monitoring")
    print("=" * 35)
    
    from hd_compute import HDComputePython
    
    # Create monitor with low thresholds to trigger alerts
    monitored_hdc = MonitoredHDC(
        HDComputePython, 
        dim=2000, 
        enable_monitoring=True
    )
    
    # Override thresholds for demo
    monitored_hdc.monitor.alert_thresholds = {
        'max_latency_ms': 50.0,  # Very low threshold
        'max_error_rate': 0.01,  # Very low threshold
        'max_memory_mb': 5.0,    # Very low threshold
        'min_ops_per_second': 100.0  # Very high threshold
    }
    
    print("Running stress operations to trigger alerts...")
    
    # Generate many operations quickly
    start_time = time.time()
    for i in range(50):
        try:
            if i % 10 == 0:
                # Introduce some errors
                monitored_hdc.bundle([])
            else:
                hv = monitored_hdc.random_hv()
                # Larger bundles to increase latency
                hvs = [monitored_hdc.random_hv() for _ in range(10)]
                bundled = monitored_hdc.bundle(hvs)
        except Exception:
            pass  # Ignore errors for stress test
    
    elapsed = time.time() - start_time
    print(f"âœ“ Completed stress test in {elapsed:.2f}s")
    
    # Check triggered alerts
    health_status = monitored_hdc.get_health_status()
    print(f"âœ“ Final status: {health_status['current_status']}")
    print(f"âœ“ Alerts triggered: {health_status['recent_alerts_count']}")
    
    for alert in health_status['recent_alerts']:
        severity_icon = "ğŸš¨" if alert['severity'] == 'critical' else "âš ï¸"
        print(f"  {severity_icon} {alert['alert_type']}: {alert['message']}")
    
    return health_status['recent_alerts_count'] > 0

if __name__ == "__main__":
    print("ğŸ“Š Starting Comprehensive Monitoring Tests...")
    
    success = True
    success &= run_monitoring_demo()
    alert_triggered = run_stress_test()
    
    if success and alert_triggered:
        print("\nâœ… All monitoring tests passed!")
        print("Comprehensive monitoring system is working correctly.")
    elif success:
        print("\nâš  Monitoring tests passed but no alerts triggered in stress test.")
    else:
        print("\nâŒ Some monitoring tests failed!")
    
    print("\nğŸ“Š Monitoring Features:")
    print("  - Real-time performance metrics collection")
    print("  - Health status determination and alerting")
    print("  - Configurable alert thresholds")
    print("  - Metrics history and trend analysis")
    print("  - JSON export for external monitoring")
    print("  - Comprehensive logging and error tracking")
    print("  - Production-ready health checks")